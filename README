Скрипт написан на python3.4. Алгоритм основан на использовании фреймворка Grub, который в свою учередь требует,
чтобы на машине, на которой он исполняется были установлены pucurl и lxml.

Выбор Grub обусловлен тем, что он позволяет авторизоваться на ресурсах и эмулировать сессию пользователя (см. развитие).
Модуль Grab:spider автоматически пытается исправить сетевые ошибки. В случае network timeout он выполняет задание ещё раз.
Количество попыток может настраиваться с помощью 'network_try_limit' опции при создании Spider объекта.

Алгоритм основан на разметке сайтов блочными тегами.

Константы и настройки.
файл settings - файл настроек селектора для ресурса (далее "частные настройки")
cur_dir - получение текущей директории, необходимое для выполнения усложнения задачи #1 (далее "текущая директория")
default_selectors_config - стандартный настройки селектора. Используются в тех случаях, когда файл настроек селектора
не найден или настройки в файле не валидны (далее "настройки по-умолчанию")

Алгритм запускается из командной строки командой python main.py <URL list>. Таким образом, алгоритм принимает в качестве
параметров один или несколько url'ов на статьи. Изначально, проводится проверка на наличие входных параметров. Если ни
один параметр не был передан в скрипт, то он завершает свою работу и сообщает о том, что не было получено
ни одного параметра. Если параметры были переданы, то происходит чтение настроек селектора из файла настроек, если таковой
имеется, иначе за настройки селектора принимаются настройки по-умолчанию (настройки селектора предвтавляют собой xpath
выражения). Далее инициализируется класс парсера, в который передается список url'ов и настройки селектора. Генерируются
задания парсера, при создании которых производится валидация url'а по регулярному выражению - если валидация не прошла,
то задание не создается и выводится сообщение о том, что url не валиден.

Алгоритм задания парсера построен следующим образом:
Если статья была ранее распарсена, то выводится предупреждение об этом и завершается работа задания, иначе происходит
валидация селектора. Если он валиден и в файле настроек селектора есть настройки для текущего ресурса, то берутся частные
настройки, иначе настройки по-умолчанию. После чего происходит парсинг атрибутов статьи в соответсвии с xpath выражениями
селектора. Распарсеные блоки записываются в файл по правилу:
http://lenta.ru/news/2013/03/dtp/index.html => [CUR_DIR]/lenta.ru/news/2013/03/dtp/index.txt

Направление дальнейшего улучшения/развития программы:

Улучшения:
1. Доработка получения входных параметров - передавать в качестве параметров не один(несколько урлов),
а файл со списком урлов для обработки алгоритмом (Представленный алгоритм уже может работать со списком урлов).
2. Доработка алгоритма валидации конфигурации селектора. На данный момент валидация заключается только в том,
что проверяется наличие всех параметров и их значений. Требуется сделать валидацию значений - xpath выражений.
3. Доработка алгоритма разбора текста. Алгоритм, ориентированный исключительно на теги парсит с полезной
информацией еще и определнную (хоть и небольшую) долю мусора.

Развитие:
1. Доработка алгоритма на возможность авторизации на ресурсах (фреймфорк Grab позволяет это сделать).
2. Реализация заданий парсинга ресурсов другого типа (форумы, блоги и т.д.)